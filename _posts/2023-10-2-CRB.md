# CRB

>Cramér-Rao bound是一种描述无偏估计器方差下界的方法，它需要一些先验知识，比如**概率论**，**统计学**，**似然函数**，**Fisher信息量**等¹²。
(1) [Cramér-Rao bound(一) - 知乎.](https://zhuanlan.zhihu.com/p/275385194)
(2) [【学习笔记】Cramer-Rao Lower Bound 克拉美-罗界](https://blog.csdn.net/qq_37851620/article/details/104049363)
(3) [克拉默-拉奥下界（Cramér-Rao lower bound） - 知乎 - 知乎专栏](https://zhuanlan.zhihu.com/p/596388612)

>- 有偏估计器存在的意义是什么?
>有偏估计器是一种估计参数的方法，它的特点是**估计量的期望值不等于参数的真实值**¹³。也就是说，有偏估计器会系统地高估或低估参数。有偏估计器存在的意义是**在某些情况下，它们可能比无偏估计器更有效或更方便**²。比如，极大似然估计就是一种有偏估计器，(样本趋于无穷时无偏)它可以用来估计正态分布的方差¹。

>当样本数量趋于无穷时，极大似然估计的期望值等于参数的真实值²³。但是，在有限样本的情况下，极大似然估计**可能**是有偏的，这取决于参数的分布和形式³⁴。一般来说，如果参数是**线性**的，那么极大似然估计是无偏的；如果参数是**非线性**的，那么极大似然估计可能是有偏的⁴。
(1) [第一课.极大似然估计与有偏性无偏性_证明极大似然估计是无偏估计_tzc_fly的博客-CSDN博客](https://bing.com/search?q=%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1%e6%97%a0%e5%81%8f%e7%9a%84%e6%9d%a1%e4%bb%b6)
(2) [最大似然估计及估计量的无偏性 - 知乎 - 知乎专栏](https://zhuanlan.zhihu.com/p/416398528)
(3) [在什么条件下，最大似然估计值是无偏估计？ - 知乎](https://www.zhihu.com/question/35670078)
(4) [第一课.极大似然估计与有偏性无偏性_证明极大似然估计是无偏估计_tzc_fly的博客-CSDN博客](https://bing.com/search?q=%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1%e6%97%a0%e5%81%8f%e7%9a%84%e6%9d%a1%e4%bb%b6)
(5) [最大似然估计及估计量的无偏性_极大似然估计的无偏性_Sakura_Logic的博客-CSDN博客](https://blog.csdn.net/Sakura_Logic/article/details/106504504)

>Fisher信息量是一种衡量样本数据携带的关于参数的信息量的方法¹²。它可以用来评估参数估计的准确性和效率³⁴。Fisher信息量的定义是**对数似然函数的二阶导数的期望值的负数**¹⁴。如果有多个观测量和多个参数，那么Fisher信息量是一个**矩阵**，每个元素表示不同参数之间的信息量⁵⁶。你想了解更多吗？
(1) Fisher信息量_鬼道2022的博客-CSDN博客. https://blog.csdn.net/qq_38406029/article/details/124582064.
(2) Fisher Information学习笔记_懒人元的博客-CSDN博客. https://blog.csdn.net/lanran2/article/details/77995062.
(3) 费希尔信息_百度百科. https://baike.baidu.com/item/%E8%B4%B9%E5%B8%8C%E5%B0%94%E4%BF%A1%E6%81%AF/22742376.
(4) Fisher信息量的直观理解与MLE的渐近正态性 - 知乎. https://zhuanlan.zhihu.com/p/376717849.
(5) 由浅入深理解 Fisher 信息（3）——从10种不同的角度和深度理解 Fisher information - 知乎. https://zhuanlan.zhihu.com/p/589321752.
(6) 由浅入深理解 Fisher 信息（2）——从10种不同的角度和深度理解 Fisher information - 知乎. https://zhuanlan.zhihu.com/p/589311732.

>计算Fisher信息量的方法有以下几个步骤²⁴：
>1. 确定样本数据服从的概率分布**f (X;θ)**，其中θ是待估计的参数。
>2. 求出该分布的**对数似然函数**L (X;θ) = log f (X;θ)。
>3. 求出对数似然函数对参数θ的**一阶导数**S (X;θ) = ∂L (X;θ) / ∂θ，称为Score函数。
>4. 求出Score函数的**方差**I (θ) = Var[S (X;θ)]，即为Fisher信息量。
(1) Fisher Information（费雪信息） - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/625170845.
(2) 如何求费希尔信息量 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/427504845.
(3) Fisher信息与Fisher信息矩阵_不讲魔法讲道理的博客-CSDN博客. https://bing.com/search?q=%e8%ae%a1%e7%ae%97Fisher%e4%bf%a1%e6%81%af%e9%87%8f%e7%9a%84%e6%96%b9%e6%b3%95.
(4) 由浅入深理解 Fisher 信息（2）——从10种不同的角度和深度理解 Fisher information - 知乎. https://zhuanlan.zhihu.com/p/589311732.
(5) 浅析Fisher 信息 - 知乎. https://zhuanlan.zhihu.com/p/589632140.
>最后一步求方差是什么含义?
方差是用来**衡量随机变量和其数学期望之间的偏离程度**的量¹³。通俗来说，就是用来**衡量随机变量的波动程度**，方差越大，那么这一组数据的波动幅度也就越大，稳定性就越小³。在Fisher信息量的计算中，求方差是为了**反映参数θ的不确定性**，方差越小，说明参数θ越容易被估计⁴。
(1) 标准差、方差、协方差三者的表示意义 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/137158563.
(2) 带你深入理解期望、方差、协方差的含义_胤风的博客-CSDN博客. https://blog.csdn.net/MoreAction_/article/details/106230690.
(3) 方差、标准差、均方差、均方误差（MSE）区别总结 - 知乎. https://zhuanlan.zhihu.com/p/83410946.
(4) 概率论2.4-期望/方差的概念及实际意义 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/569201508.
(5) 方差_百度百科. https://baike.baidu.com/item/方差/3108412.

>Fisher信息量与Cramér-Rao bound什么关系?
Fisher信息量与Cramér-Rao bound的关系是，**Fisher信息量的倒数是无偏估计量的方差的下界**¹³。也就是说，任何无偏估计量的方差都不会小于Fisher信息量的倒数，而等号成立的条件是估计量是**有效估计量**¹。这个不等式可以用来**评价估计量的优劣**，也可以用来**寻找有效估计量**¹。
(1) Fisher信息量与Cramer-Rao不等式_走过了笔尖的博客-CSDN博客. https://blog.csdn.net/Queen0911/article/details/101487935.
(2) 由浅入深理解 Fisher 信息（3）——从10种不同的角度和深度理解 Fisher information - 知乎. https://zhuanlan.zhihu.com/p/589321752.
(3) 现代信号处理——参数估计理论（Fisher信息与克拉美罗（Cramer-Rao）下界）_克拉美罗下界_清泉_流响的博客-CSDN博客. https://blog.csdn.net/qq_42233059/article/details/127072340.

>这个结论的证明需要用到**对数似然函数**和**施瓦茨不等式**¹²。证明的大致思路是，首先将无偏估计量的方差写成对数似然函数的二阶导数的期望的形式，然后利用施瓦茨不等式给出一个下界，最后化简得到Fisher信息量的倒数¹²。这个结论说明了**观测数据中包含的参数信息越多，估计量的方差就越小**³。你想看具体的证明过程吗？
(1) Cramér-Rao bound(一) - 知乎. https://zhuanlan.zhihu.com/p/275385194.
(2) Fisher信息量与Cramer-Rao不等式_走过了笔尖的博客-CSDN博客. https://blog.csdn.net/Queen0911/article/details/101487935.
(3) 【学习笔记】Cramer-Rao Lower Bound 克拉美-罗界 - CSDN博客. https://blog.csdn.net/qq_37851620/article/details/104049363.

>这个结论是不是可以说明最大似然估计是最粗略的估计手段?
最大似然估计不一定是最粗略的估计手段，它有自己的优点和缺点。最大似然估计的优点是：
>- **比其他估计方法更加简单**¹；
>- **收敛性好**，无偏或者渐近无偏，当样本数目增加时，收敛性质会更好¹²；
>- **如果假设的类条件概率模型正确，则通常能获得较好的结果**¹。
>最大似然估计的缺点是：
>- **计算量较大**，需要求解对数似然函数的极值²；
>- **对模型的假设敏感**，如果模型假设错误，则可能导致估计失效³⁴；
>- **可能出现过拟合现象**，即模型过于贴合样本数据而忽略了噪声的影响⁵。
(1) 机器学习笔记1.矩估计、极大似然估计。_矩估计和极大似然估计优缺点_听闻远方有佳人的博客-CSDN博客. https://blog.csdn.net/qq_44607404/article/details/102221990.
(2) 最大似然估计详解 - 很随便的wei - 博客园. https://www.cnblogs.com/boostwei/p/14867987.html.
(3) 如何深刻理解的最大似然估计 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/148968222.
(4) 参数估计(二).最大似然估计 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/55791843.
(5) 一文搞懂极大似然估计 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/26614750.

---
>极大似然估计估计得到的参数估计的方差比其他估计方法得到的方差都要大吗?
这个问题没有一个确定的答案，因为不同的估计方法有不同的性质和适用条件。一般来说，极大似然估计得到的方差估计是**有偏的**¹²，也就是说它的期望值不等于真实的方差值。但是，当样本量足够大时，极大似然估计的方差估计会**渐近无偏**²，也就是说它的期望值会趋近于真实的方差值。另外，极大似然估计还具有**渐近正态性**和**渐近最小方差性**³⁴，也就是说当样本量足够大时，极大似然估计会服从一个以真实参数为均值、以信息量为方差的正态分布，并且这个方差是所有无偏估计中最小的。所以，极大似然估计在样本量较小的情况下可能不如其他方法精确，但在样本量较大的情况下可能比其他方法更优。你想要比较哪些方法呢？
源: 与必应的对话， 2023/9/10
(1) 为什么极大似然估计得到的方差是有偏估计 - CSDN博客. https://blog.csdn.net/smallflyingpig/article/details/52964209.
(2) 正态分布下的最大似然估计_正态分布方差的最大似然估计_DeeGLMath的博客-CSDN博客. https://blog.csdn.net/linjing_zyq/article/details/120380707.
(3) 一文搞懂极大似然估计 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/26614750.
(4) 数理统计第十讲（极大似然估计步骤与性质） - 知乎专栏. https://zhuanlan.zhihu.com/p/152614797.
>如何判断一个参数是否渐近正态分布？
判断一个参数是否渐近正态分布，一般有以下几种方法¹²：
>- **中心极限定理**：如果一个参数是由大量独立同分布的随机变量的函数，那么当样本量足够大时，这个参数的分布会趋近于正态分布。
>- **渐近正态估计**：如果一个参数的估计量在样本量趋于无穷时，服从以真实参数为均值、以信息量为方差的正态分布，那么这个参数的估计量具有渐近正态性。
>- **费希尔信息阵**：如果一个参数的估计量满足一些正则条件，那么它的方差矩阵的逆矩阵就是费希尔信息阵，可以用来判断渐近正态性。
>- **Von-Mises展开**：如果一个参数是未知分布函数的一个泛函，那么它的估计量可以用Von-Mises展开来近似，从而判断渐近正态性。
(1) 渐近正态估计 - 百度百科. https://baike.baidu.com/item/%E6%B8%90%E8%BF%91%E6%AD%A3%E6%80%81%E4%BC%B0%E8%AE%A1/22676160.
(2) 渐近正态性 - 百度百科. https://baike.baidu.com/item/%E6%B8%90%E8%BF%91%E6%AD%A3%E6%80%81%E6%80%A7/19098406.
(3) 数理统计第11讲（MLE渐近正态性，最小二乘，一致最小方差） - 知乎. https://zhuanlan.zhihu.com/p/153127028.
(4) 渐近正态性 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/456881138.
(5) 杂记——极大似然估计的渐近正态性 - CSDN博客. https://blog.csdn.net/TSzero/article/details/112135446.

---
>信息量为什么可以用方差表示?
信息量和方差的关系并不是一般的，而是依赖于具体的分布和应用场景。一般来说，有以下几种情况¹²：
>- **主成分分析**：在这种方法中，方差被用作衡量信息量的指标，因为方差越大说明数据越具有多样性，相关性也就越强。
>- **信息熵**：在信息论中，信息熵是信息的度量，它和不确定度是等价的。在一些特殊的分布下，如高斯分布或伯努利分布，方差和信息熵是正相关的，也就是说方差越大，信息熵也越大。
>- **费希尔信息量**：在数理统计中，费希尔信息量是衡量参数估计效率的指标，它和方差有一个反比的关系，即费希尔信息量越大，方差越小。这意味着信息量越大，估计量越精确。
源: 与必应的对话， 2023/9/10
(1) 如何理解“方差越大信息量就越多”？ - 知乎. https://www.zhihu.com/question/36481348.
(2) 如何理解“方差越大信息量就越多”_方差越大越好的例子_小壁虎的春天的博客-CSDN博客. https://blog.csdn.net/zk168_net/article/details/103697934.
(3) 主成分分析(PCA)的详细解释 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/58663947.
(4) 数理统计第14讲（UMVUE典型难题，Fisher信息量，C-R不等式） - 知乎. https://zhuanlan.zhihu.com/p/154336015.

---

>请你举几个例子 什么时候信息量与方差没关系,并给出原因.
信息量与方差无关的例子有以下几种¹²³：
>- **密度函数与参数无关**：如果一个参数的密度函数与参数无关，那么它的费希尔信息量就为0，也就是说信息量与方差没有任何关系。
>- **非指数族分布**：如果一个参数的分布不属于指数族分布，那么它的费希尔信息量就不一定存在或唯一，也就是说信息量与方差没有确定的关系。
>- **非无偏估计**：如果一个参数的估计量不是无偏的，那么它的方差就不能反映它的效率，也就是说信息量与方差没有直接的关系。
源: 与必应的对话， 2023/9/10
(1) 数理统计第14讲（UMVUE典型难题，Fisher信息量，C-R不等式） - 知乎. https://zhuanlan.zhihu.com/p/154336015.
(2) 数理统计|笔记整理（5）——估计量的进阶性质（1）：统计判决函数，UMRUE，Fisher信息量 - 知乎. https://zhuanlan.zhihu.com/p/89868315.
(3) 数理统计（4）——参数点估计的优良性及费希尔信息量 - 知乎. https://zhuanlan.zhihu.com/p/359364147.

>在所有分布中的信息量的计算中,哪一点原则是始终不变的?
信息量的计算中，有一个始终不变的原则，就是**信息量与概率成反比**¹⁴。也就是说，一个事件发生的概率越小，它所包含的信息量就越大，反之亦然。这个原则可以用数学公式表示为：
$$
I(x) = -\log P(x)
$$
其中，$I(x)$是事件$x$的信息量，$P(x)$是事件$x$的概率，$\log$是对数函数。这个公式可以用不同的底数来计算，常见的有2（比特）、10（达尔）、e（纳特）。
源: 与必应的对话， 2023/9/10
(1) 信息度量及信息有效性计算（精简版）——通信原理笔记（一）_通信原理什么是信息量_傻萌的路飞的博客-CSDN博客. https://blog.csdn.net/backlight__/article/details/120913112.
(2) 什么是信息量？如何计算信息量？ - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/106526229.
(3) 信息熵_百度百科. https://baike.baidu.com/item/信息熵/7302318.
(4) 信息量公式 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/97994025.
(5) 详解：信息量、信息熵、交叉熵、相对熵 - 知乎. https://zhuanlan.zhihu.com/p/624885363.